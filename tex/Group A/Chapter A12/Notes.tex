\section{Notes}

\subsection{Basic Terminology}

\begin{definition}
    A statistical or random \vocab{experiment} (or trial) refers to a process that generates a set of observable outcomes, and can be repeated under the same set of conditions.
\end{definition}

\begin{definition}
    The \vocab{sample space} (or possibility space) $S$ of an experiment is the set of all possible outcomes of the experiment.
\end{definition}

\begin{definition}
    An \vocab{event} $E$ is a subset of $S$. The \vocab{complement} of $E$, denoted by $E'$, is the event that $E$ does not occur, i.e. $E' = S \setminus E$.
\end{definition}

\begin{definition}
    Given a subset $G \subseteq S$, the function $n(G)$ returns the \vocab{number of possible outcomes} in $G$.
\end{definition}

\subsection{Probability}

\begin{definition}[Classical Probability]
    If the sample space $S$ consists of a finite number of equally likely outcomes, then the probability of an event $E$ occuring (a measure of the likelihood that $E$ occurs) is denoted $\P{E}$ and is defined as \[\P{E} = \frac{n(E)}{n(S)}.\]
\end{definition}

\begin{proposition}[Range of Probabilities]
    For any event $E$, \[\P{E} \in [0, 1].\]
\end{proposition}
\begin{proof}
    Let the sample space be $S$. Since $E \subseteq S$, we have \[0 \leq n(E) \leq n(S) \implies 0 \leq \frac{n(E)}{n(S)} \leq \frac{n(S)}{n(S)} \implies 0 \leq \P{E} \leq 1.\]
\end{proof}

\begin{corollary}
    Let $A$ and $B$ be two events. If $A \subseteq B$, then $\P{A} \leq \P{B}$.
\end{corollary}
\begin{proof}
    Identical as above.
\end{proof}

\begin{definition}
    When $\P{E} = 0$, we say that $E$ is an \vocab{impossible} event. When $\P{E} = 1$, we say that $P$ is a \vocab{sure} event.
\end{definition}

\begin{proposition}[Probability of Complement]
    For any event $E$, \[\P{E} + \P{E'} = 1.\]
\end{proposition}
\begin{proof}
    Let the sample space be $S$. By definition, $E' = S \setminus E$. Hence, \[n(E') = n(S) - n(E) \implies \frac{n(E)}{n(S)} + \frac{n(E')}{n(S)} = \frac{n(S)}{n(S)} \implies \P{E} + \P{E'} = 1.\]
\end{proof}

\begin{definition}
    Let $S$ be the sample space of a random experiment and $A$, $B$ be any two events.
    \begin{itemize}
        \item The \vocab{intersection} of $A$ and $B$, denoted by $A \cap B$, is the event that both $A$ and $B$ occur.
        \item The \vocab{union} of $A$ and $B$, denoted by $A \cup B$, is the event that at least one occurs.
    \end{itemize}
\end{definition}

\begin{proposition}[Inclusion-Exclusion Principle]
    Let $A$ and $B$ be any two events in a sample space $S$. Then \[\P{A \cup B} = \P{A} + \P{B} - \P{A \cap B}.\]
\end{proposition}
\begin{proof}
    When we take the sum of the number of outcomes in events $A$ and $B$, i.e. $n(A) + n(B)$, we will count the `overlap', i.e. $n(A \cap B)$, twice. Hence, \[n(A \cup B) = n(A) + n(B) - n(A \cap B).\] Dividing throughout by $n(S)$ yields the desired result.
\end{proof}

\begin{proposition}[Intersection of Complements]
    Let $A$ and $B$ be two events. Then \[\P{A} = \P{A \cap B} + \P{A \cap B'}.\]
\end{proposition}
\begin{proof}
    By definition, $B' = S \setminus B$. Taking the intersection with $A$ on both sides, \[\P{A \cap B'} = \P{A \cap S} - \P{A \cap B} \implies \P{A \cap B} + \P{A \cap B'} = \P{A}.\]
\end{proof}

\begin{proposition}[``Neither Nor'']
    Let $A$ and $B$ be two events. Then \[\P{A' \cap B'} = 1 - \P{A \cup B}.\]
\end{proposition}
\begin{proof}
    In layman terms, the above statement translates to \[\P{\text{neither $A$ nor $B$}} = 1 - \P{\text{$A$ or $B$}},\] which is clearly true.
\end{proof}

\subsection{Mutually Exlusive Events}

\begin{definition}
    Two events $A$ and $B$ are said to be \vocab{mutually exclusive} if they cannot occur at the same time. Mathematically, \[\P{A \cap B} = 0.\]
\end{definition}

An equivalent criterion for mutual exclusivity is \[\P{A \cup B} = \P{A} + \P{B},\] which can easily be derived from $\P{A \cap B} = 0$ via the inclusion-exclusion principle.

\subsection{Conditional Probability and Independent Events}

